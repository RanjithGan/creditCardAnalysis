{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8e2aeb-2b38-4bd9-afe5-c4a97b39325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/21 22:58:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# session creation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreditCards\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b32f73d5-f5a3-4504-809e-22e1b7ccf2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count before removing duplicates 569614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count after removing duplicates 283726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 682:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----------------+----------------+-----------------+-----------------+----------------+-----------------+---------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+\n",
      "|Time|                V1|               V2|              V3|               V4|               V5|              V6|               V7|             V8|                V9|               V10|              V11|              V12|               V13|              V14|              V15|               V16|               V17|               V18|               V19|               V20|               V21|              V22|               V23|                V24|               V25|               V26|                V27|               V28|Amount|Class|\n",
      "+----+------------------+-----------------+----------------+-----------------+-----------------+----------------+-----------------+---------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+\n",
      "|26.0|-0.529912284186556|0.873891581460326|1.34724732930113|0.145456676582257|0.414208858362661|0.10022309405219|0.711206082959649|0.1760659570625|-0.286716934699997|-0.484687683196852|0.872489590125871|0.851635859904339|-0.571745302934562|0.100974273045751|-1.51977183258512|-0.284375978261788|-0.310523584869201|-0.404247868800905|-0.823373523914155|-0.290347610865436|0.0469490671140629|0.208104855076299|-0.185548346773547|0.00103065983293288|0.0988157011025622|-0.552903603040518|-0.0732880835681738|0.0233070451077205|  6.14|    0|\n",
      "+----+------------------+-----------------+----------------+-----------------+-----------------+----------------+-----------------+---------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Reading csv files, de duplicate\n",
    "\n",
    "# Wildcard loading , creates single DF from multiple files, loads data in parallel.\n",
    "df = spark.read.format('csv').option(\"header\", \"true\").option('InferSchema',True).load('/home/ranjith/Downloads/archive/*csv')\n",
    "\n",
    "print('count before removing duplicates',df.count())\n",
    "\n",
    "# removing duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "print('count after removing duplicates',df.count())\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88d0432-ed94-4ebf-86e8-e5558805859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF having 283726 rows and 31 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "# Size of the DF\n",
    "\n",
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "\n",
    "print(f\"DF having {rows} rows and {cols} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08a0753c-03d9-45fb-a677-9053cefb15ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# checking for null / missing values\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3faadd7-0842-4983-93f5-471918a0a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|summary|              Time|                  V1|                  V2|                  V3|                  V4|                  V5|                  V6|                  V7|                  V8|                  V9|                 V10|                 V11|                 V12|                 V13|                 V14|                 V15|                 V16|                 V17|                 V18|                 V19|                 V20|                 V21|                 V22|                 V23|                 V24|                 V25|                 V26|                 V27|                 V28|            Amount|               Class|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|  count|            283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|              283726|            283726|              283726|\n",
      "|   mean| 94811.07759951502|0.005917149836166...|-0.00413475562812...|0.001613119355878...|-0.00296630772034...|0.001827560113032847|-0.00113948818973...|0.001800691765307...|-8.54452573453876...|-0.00159619962170...|-0.00144071048503...|2.017576399587971...|-7.14787657405844E-4|6.033757913237318E-4|2.523173005216604E-4|0.001042838049814...|0.001162012815673...|1.701609445485151...|0.001515166010458...|-2.64263587519032...|1.871752473165260...|-3.70593123359157...|-1.50275631649892...|1.981707238115089...|2.142068242224124...|-2.32386989607557E-4|1.494410878995642...|0.001763031617330...|5.473121010899007E-4| 88.47268731101789|0.001667101358352777|\n",
      "| stddev|47481.047890619346|  1.9480261416254743|  1.6467029642463489|  1.5086819162059166|   1.414184014447514|  1.3770082792800915|  1.3319305917151598|  1.2276638954422607|  1.1790544275788073|  1.0954924810736137|  1.0764073501381097|  1.0187201526753222|  0.9946744452140558|  0.9954296367703033|  0.9522150900331066|  0.9148936334525594|  0.8736963275712365|  0.8425073207466568|  0.8373775297813788|  0.8133785531048054|  0.7699842414107966|  0.7239093668620514|  0.7245504654239998|   0.623702378580731|  0.6056266981643231|  0.5212203166871472| 0.48205294082758093|  0.3957438809320338| 0.32802660429357794|250.39943711577322|0.040796176259338485|\n",
      "|    min|               0.0|    -56.407509631329|   -72.7157275629303|   -48.3255893623954|   -5.68317119816995|   -113.743306711146|   -26.1605059358433|   -43.5572415712451|   -73.2167184552674|   -13.4340663182301|   -24.5882624372475|   -4.79747346479757|   -18.6837146333443|   -5.79188120632084|   -19.2143254902614|   -4.49894467676621|   -14.1298545174931|   -25.1627993693248|   -9.49874592104677|   -7.21352743017759|    -54.497720494566|   -34.8303821448146|    -10.933143697655|   -44.8077352037913|   -2.83662691870341|   -10.2953970749851|   -2.60455055280817|   -22.5656793207827|   -15.4300839055349|               0.0|                   0|\n",
      "|    max|          172792.0|    2.45492999121121|    22.0577289904909|    9.38255843282114|    16.8753440335975|    34.8016658766686|    73.3016255459646|    120.589493945238|    20.0072083651213|    15.5949946071278|    23.7451361206545|    12.0189131816199|     7.8483920756446|    7.12688295859376|    10.5267660517847|    8.87774159774277|    17.3151115176278|    9.25352625047285|    5.04106918541184|    5.59197142733558|    39.4209042482199|    27.2028391573154|    10.5030900899454|    22.5284116897749|    4.58454913689817|    7.51958867870916|     3.5173456116238|    31.6121981061363|    33.8478078188831|          25691.16|                   1|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 691:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+------------------+\n",
      "|      Mean_Amount|Min_Amount|Max_Amount|     StdDev_Amount|\n",
      "+-----------------+----------+----------+------------------+\n",
      "|88.47268731101789|       0.0|  25691.16|250.39943711577322|\n",
      "+-----------------+----------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Statistics \n",
    "from pyspark.sql.functions import col, mean, stddev, min, max\n",
    "\n",
    "# column wise statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Amount column statistics\n",
    "df.select(\n",
    "    mean(\"Amount\").alias(\"Mean_Amount\"),\n",
    "    min(\"Amount\").alias(\"Min_Amount\"),\n",
    "    max(\"Amount\").alias(\"Max_Amount\"),\n",
    "    stddev(\"Amount\").alias(\"StdDev_Amount\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "218b68c5-0ab2-406e-abd3-03b8c94e4b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 567:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median for Time: 84681.0\n",
      "Median for V1: 0.0201435755087573\n",
      "Median for V2: 0.0637941997421073\n",
      "Median for V3: 0.179871147511258\n",
      "Median for V4: -0.0224219695555284\n",
      "Median for V5: -0.0536312510232358\n",
      "Median for V6: -0.275358573562355\n",
      "Median for V7: 0.0407401875880306\n",
      "Median for V8: 0.0218575949616256\n",
      "Median for V9: -0.0525956513955632\n",
      "Median for V10: -0.0933317436055039\n",
      "Median for V11: -0.0323385470334912\n",
      "Median for V12: 0.138996143050385\n",
      "Median for V13: -0.0129937551085427\n",
      "Median for V14: 0.0500901541828535\n",
      "Median for V15: 0.049184212432686\n",
      "Median for V16: 0.0670268080034959\n",
      "Median for V17: -0.0659678076478633\n",
      "Median for V18: -0.0022405035283073\n",
      "Median for V19: 0.00330335625186285\n",
      "Median for V20: -0.0623879623178263\n",
      "Median for V21: -0.0295078434834623\n",
      "Median for V22: 0.00666190163775685\n",
      "Median for V23: -0.0111704465132397\n",
      "Median for V24: 0.0409487542693962\n",
      "Median for V25: 0.0162138522088198\n",
      "Median for V26: -0.0522052302313139\n",
      "Median for V27: 0.00145992468138233\n",
      "Median for V28: 0.0112768909078317\n",
      "Median for Amount: 22.0\n",
      "Median for Class: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# median of the columns using percentile_approx function (this is suitable for large data).\n",
    "\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "#function to calculate median of each column.\n",
    "def calculate_median_spark_native(df, column):\n",
    "    # null values filtered\n",
    "    median = df.filter(col(column).isNotNull()).select(\n",
    "        expr(f\"percentile_approx({column}, 0.5)\").alias(\"median\")\n",
    "    ).collect()[0][\"median\"]\n",
    "    return median\n",
    "\n",
    "# creating empty dictionary\n",
    "median_results = {}\n",
    "\n",
    "# Calculate median for every column\n",
    "for column in df.columns:\n",
    "    median_results[column] = calculate_median_spark_native(df, column)\n",
    "\n",
    "# Output the median values for each column\n",
    "for column, median in median_results.items():\n",
    "    print(f\"Median for {column}: {median}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de775e10-2142-43e2-9957-28912ccc5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 877:==============>                                          (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Time', 3767.0), ('V1', 2.05579700630039), ('V2', 0.166975019545401), ('V3', 0.488305742562781), ('V4', 0.6353219207244), ('V5', -0.562776680773863), ('V6', -1.01107261632698), ('V7', 0.0149526614685896), ('V8', -0.160210863301812), ('V9', 0.608605870267216), ('V10', -0.0445745893804268), ('V11', -0.35674901847752), ('V12', 0.350563573253678), ('V13', -0.517759694198053), ('V14', 0.690971618395625), ('V15', 1.2752570390935), ('V16', 0.342469754110769), ('V17', -0.601956802828445), ('V18', -0.0526401462570187), ('V19', -0.330590448442944), ('V20', -0.180370118559693), ('V21', 0.269764951361357), ('V22', -0.816263763157847), ('V23', 0.140304302014326), ('V24', 0.726211883811499), ('V25', 0.366624307004913), ('V26', -0.398827514959463), ('V27', 0.0277351215052822), ('V28', 0.0184945729704665), ('Amount', 1.0), ('Class', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# mode of columns\n",
    "from pyspark.sql.functions import col,sum\n",
    "\n",
    "# creating empty list\n",
    "mod_col_data = []\n",
    "\n",
    "# looping through every column to get the mode of that column\n",
    "for col_name in df.columns:\n",
    "    mode_df = df.groupBy(col(col_name)).count().orderBy('count',ascending=False).limit(1)\n",
    "    mode = mode_df.collect()[0][0]\n",
    "    mod_col_data.append((col_name,mode))\n",
    "\n",
    "print(mod_col_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52c5fb7a-188c-4bce-b89e-6698f560ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 636:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----------------+----------------+-----------------+-----------------+----------------+-----------------+---------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+\n",
      "|Time|                V1|               V2|              V3|               V4|               V5|              V6|               V7|             V8|                V9|               V10|              V11|              V12|               V13|              V14|              V15|               V16|               V17|               V18|               V19|               V20|               V21|              V22|               V23|                V24|               V25|               V26|                V27|               V28|Amount|Class|\n",
      "+----+------------------+-----------------+----------------+-----------------+-----------------+----------------+-----------------+---------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+\n",
      "|26.0|-0.529912284186556|0.873891581460326|1.34724732930113|0.145456676582257|0.414208858362661|0.10022309405219|0.711206082959649|0.1760659570625|-0.286716934699997|-0.484687683196852|0.872489590125871|0.851635859904339|-0.571745302934562|0.100974273045751|-1.51977183258512|-0.284375978261788|-0.310523584869201|-0.404247868800905|-0.823373523914155|-0.290347610865436|0.0469490671140629|0.208104855076299|-0.185548346773547|0.00103065983293288|0.0988157011025622|-0.552903603040518|-0.0732880835681738|0.0233070451077205|  6.14|    0|\n",
      "+----+------------------+-----------------+----------------+-----------------+-----------------+----------------+-----------------+---------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "from pyspark.sql.functions import col,sum\n",
    "\n",
    "#dropping rows having nulls\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# removing duplicate records\n",
    "df_cleaned = df_cleaned.dropDuplicates()\n",
    "\n",
    "df_cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c74b94f-1ce1-41b8-951b-7e6eaf9a091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 651:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Amount|    NormalizedAmount|\n",
      "+------+--------------------+\n",
      "|  6.14|-0.32880540092009486|\n",
      "+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Data Transformation: NormalizedAmount column added.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Amount column statistics\n",
    "df_temp = df_cleaned.select(\n",
    "    mean(\"Amount\").alias(\"Mean_Amount\"),\n",
    "    min(\"Amount\").alias(\"Min_Amount\"),\n",
    "    max(\"Amount\").alias(\"Max_Amount\"),\n",
    "    stddev(\"Amount\").alias(\"StdDev_Amount\")\n",
    ")\n",
    "\n",
    "# extracting mean from above statistics df\n",
    "Mean_Amount = df_temp.select('Mean_Amount').collect()[0][0]\n",
    "\n",
    "# extracting standard deviation from above statistics df\n",
    "StdDev_Amount = df_temp.select('StdDev_Amount').collect()[0][0]\n",
    "\n",
    "# adding new column \"NormalizedAmount\" \n",
    "df_col_added = df_cleaned.withColumn(\"NormalizedAmount\",(col('Amount')-Mean_Amount)/StdDev_Amount)\n",
    "\n",
    "# showing sample data\n",
    "df_col_added.select('Amount','NormalizedAmount').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b026557c-942c-4bed-847c-63635a254677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 654:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|Amount|        AmountLog|\n",
      "+------+-----------------+\n",
      "|  6.14|1.965712776351493|\n",
      "+------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Mathematical Transformation: \n",
    "# adding column \n",
    "\n",
    "from pyspark.sql.functions import log, col\n",
    "\n",
    "# adding column \"AmountLog\"\n",
    "df_col2_added = df_col_added.withColumn('AmountLog',log(col('Amount')+1))\n",
    "\n",
    "# showing sample data\n",
    "df_col2_added.select('Amount','AmountLog').show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7c45e4c-f891-4e99-9458-8ef9f46f26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+----------------+----------------+-----------------+-----------------+------------------+---------------+------------------+-----------------+----------------+----------------+-----------------+----------------+-------------------+----------------+-----------------+------------------+-----------------+----------------+-----------------+-------------------+-----------------+------------------+-------------------+------------------+------------------+------------------+------------------+------+-----+--------------------+-----------------+\n",
      "|Time|                V1|              V2|              V3|               V4|               V5|                V6|             V7|                V8|               V9|             V10|             V11|              V12|             V13|                V14|             V15|              V16|               V17|              V18|             V19|              V20|                V21|              V22|               V23|                V24|               V25|               V26|               V27|               V28|Amount|Class|    NormalizedAmount|        AmountLog|\n",
      "+----+------------------+----------------+----------------+-----------------+-----------------+------------------+---------------+------------------+-----------------+----------------+----------------+-----------------+----------------+-------------------+----------------+-----------------+------------------+-----------------+----------------+-----------------+-------------------+-----------------+------------------+-------------------+------------------+------------------+------------------+------------------+------+-----+--------------------+-----------------+\n",
      "|50.0|-0.571520749961747|1.07160044939815|1.28011024961142|0.542780003052458|0.574438986516567|-0.259359265830457|1.0611484151742|-0.410971967925814|-0.17913018113825|1.00350065764887|1.06238226849436|0.159422388058099|-0.2898603137468|0.00580474635482856|0.46044095320685|-0.22103526461389|-0.775852454545776|0.657023315008206|0.80758291868891|0.354730634765855|0.00355852008044498|0.561240486431022|-0.199286719692043|0.00138738938225278|-0.179529951929609|-0.374115725753305|0.0716414539902948|-0.175510468698003|  9.79|    0|-0.31422869083622834|2.378619779270043|\n",
      "+----+------------------+----------------+----------------+-----------------+-----------------+------------------+---------------+------------------+-----------------+----------------+----------------+-----------------+----------------+-------------------+----------------+-----------------+------------------+-----------------+----------------+-----------------+-------------------+-----------------+------------------+-------------------+------------------+------------------+------------------+------------------+------+-----+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File Conversion\n",
    "\n",
    "# writing as parquet format to compress it and also to improve query performance.\n",
    "df_col2_added.write.format(\"parquet\").mode('overwrite').save('/home/ranjith/creditCardResultDf')\n",
    "\n",
    "# reading the file back to spark df.\n",
    "parquet_df = spark.read.format('parquet').load('/home/ranjith/creditCardResultDf')\n",
    "\n",
    "parquet_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16070dd2-7031-4f41-bf14-5b8e3447acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|avg_normalized_amount|\n",
      "+---------------------+\n",
      "|  0.14137081760983125|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|max_normalized_amount|\n",
      "+---------------------+\n",
      "|   102.24738365067279|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Querying: \n",
    "\n",
    "# creating temporary view to perform sql operations.\n",
    "parquet_df.createOrReplaceTempView(\"parquet_df_view\")\n",
    "\n",
    "# What is the average normalized amount in fraudulent transactions?\n",
    "\n",
    "spark.sql('''\n",
    "SELECT AVG(NormalizedAmount) AS avg_normalized_amount\n",
    "FROM PARQUET_DF_VIEW\n",
    "WHERE CLASS = '1'\n",
    "''').show()\n",
    "\n",
    "# What is the maximum normalized amount in non-fraudulent transactions?\n",
    "\n",
    "spark.sql('''\n",
    "SELECT MAX(NormalizedAmount) AS max_normalized_amount\n",
    "FROM PARQUET_DF_VIEW\n",
    "WHERE CLASS = '0'\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15f1d2-396e-4cf9-88ac-3aa0bf65bc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
